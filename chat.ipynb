{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intent.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'greeting',\n",
       "   'patterns': ['Hello',\n",
       "    'Hi',\n",
       "    'Hey',\n",
       "    'Good morning',\n",
       "    'Good afternoon',\n",
       "    'Good evening',\n",
       "    \"What's up?\",\n",
       "    \"How's it going?\",\n",
       "    'Hey there',\n",
       "    'Greetings'],\n",
       "   'responses': ['Hello! How can I assist you today?',\n",
       "    'Hi there! How can I help?',\n",
       "    'Hey! What can I do for you?',\n",
       "    'Hello! What do you need help with today?']},\n",
       "  {'tag': 'farewell',\n",
       "   'patterns': ['Bye',\n",
       "    'Goodbye',\n",
       "    'See you later',\n",
       "    \"I'm leaving\",\n",
       "    'Take care',\n",
       "    'Catch you later',\n",
       "    'See ya',\n",
       "    'Have a good day',\n",
       "    \"I'm off\",\n",
       "    'Talk to you soon'],\n",
       "   'responses': ['Goodbye! Have a great day!',\n",
       "    'See you later! Feel free to ask anytime.',\n",
       "    'Take care! Hope to talk again soon.',\n",
       "    \"Goodbye! Don't hesitate to reach out.\"]},\n",
       "  {'tag': 'product_search',\n",
       "   'patterns': ['Tell me about star earring',\n",
       "    'Tell me about heart pendant',\n",
       "    'Tell me about brown watch',\n",
       "    'Is {product_name} available?',\n",
       "    'Do you have {product_name}?',\n",
       "    'Can I get more info about {product_name}?',\n",
       "    'What is {product_name}?',\n",
       "    'Do you have any details on {product_name}?',\n",
       "    'Give me some information on {product_name}',\n",
       "    'Show me {product_name}',\n",
       "    'Is {product_name} in stock?',\n",
       "    'Where can I find {product_name}?',\n",
       "    'tell me more about {product_name}',\n",
       "    'star earring',\n",
       "    'heart pendant',\n",
       "    'black shades',\n",
       "    'heart ring',\n",
       "    'drop earring',\n",
       "    'Brown watch',\n",
       "    'finger ring'],\n",
       "   'responses': ['Let me check if we have {product_name} in stock...',\n",
       "    'Checking availability for {product_name}...',\n",
       "    \"I'll get more information about {product_name} for you.\",\n",
       "    'Looking up details for {product_name}...',\n",
       "    'Here are the details for {product_name}:',\n",
       "    'Let me find the price and availability of {product_name} for you.',\n",
       "    'Iâ€™ll fetch the information about {product_name}. Please hold on.',\n",
       "    'Let me fetch the specifications for {product_name}.',\n",
       "    'Hereâ€™s the image of {product_name} you asked for.',\n",
       "    'Let me check if we have {product_name} in your area.',\n",
       "    'Iâ€™ll recommend something similar to {product_name}. Just a moment.']},\n",
       "  {'tag': 'category_search',\n",
       "   'patterns': ['What do you have in {category_name}?',\n",
       "    'Show me products in {category_name}',\n",
       "    'What products are available in {category_name}?',\n",
       "    'List items in the {category_name} category',\n",
       "    'watch',\n",
       "    'ring',\n",
       "    'earring',\n",
       "    'bracelet',\n",
       "    'necklace',\n",
       "    'sunglass'],\n",
       "   'responses': ['Here are the products in the {category_name} category.']},\n",
       "  {'tag': 'order_status',\n",
       "   'patterns': ['Where is my order?',\n",
       "    'Track my order',\n",
       "    'What is the status of my order?',\n",
       "    'Can I get an update on my order?',\n",
       "    'Has my order shipped?',\n",
       "    'Tell me where my order is',\n",
       "    \"Where's my package?\",\n",
       "    'Has my order been dispatched?',\n",
       "    'Is my order on the way?',\n",
       "    'What is the ETA of my order?'],\n",
       "   'responses': ['Let me check the status of your order.',\n",
       "    'I will look up your order details for you.',\n",
       "    \"Your order is being processed. I'll get back to you with the status.\",\n",
       "    'Let me find the tracking information for your order.']},\n",
       "  {'tag': 'customer_service',\n",
       "   'patterns': ['I need help',\n",
       "    'Can I talk to a representative?',\n",
       "    'Is there someone I can speak to?',\n",
       "    'Can you assist me?',\n",
       "    'Can you help me out?',\n",
       "    'I have a problem',\n",
       "    'I need assistance',\n",
       "    'Get me customer support',\n",
       "    'I need customer service',\n",
       "    'Can I speak to someone?'],\n",
       "   'responses': ['Iâ€™m here to help! What do you need assistance with?',\n",
       "    'Sure! What can I help you with today?',\n",
       "    'Of course! Whatâ€™s the issue?',\n",
       "    \"I'm happy to help! How can I assist you?\"]},\n",
       "  {'tag': 'return_policy',\n",
       "   'patterns': ['What is your return policy?',\n",
       "    'How can I return a product?',\n",
       "    'Can I return an item?',\n",
       "    'Whatâ€™s the process for returning something?',\n",
       "    'How long do I have to return a product?',\n",
       "    'Do you accept returns?',\n",
       "    'Whatâ€™s your refund policy?',\n",
       "    'How can I get a refund?',\n",
       "    'Can I exchange a product?',\n",
       "    'How do I send a product back?'],\n",
       "   'responses': ['Our return policy allows returns within 30 days of purchase.',\n",
       "    'You can return any product within 30 days of delivery.',\n",
       "    'To return a product, please visit our returns page.',\n",
       "    'We accept returns for up to 30 days after purchase.']},\n",
       "  {'tag': 'payment_inquiry',\n",
       "   'patterns': ['How can I pay?',\n",
       "    'What payment methods do you accept?',\n",
       "    'Can I pay with PayPal?',\n",
       "    'Is cash on delivery available?',\n",
       "    'Do you accept credit cards?',\n",
       "    'How do I pay for my order?',\n",
       "    'What are my payment options?',\n",
       "    'Can I use my debit card?',\n",
       "    'Is payment secure?',\n",
       "    'What is the payment process?',\n",
       "    'pay',\n",
       "    'how to pay'],\n",
       "   'responses': ['We accept credit cards, debit cards, and PayPal.',\n",
       "    'You can pay using a credit or debit card, or PayPal.',\n",
       "    'Payment is secure with SSL encryption for all transactions.',\n",
       "    'We accept all major credit cards and PayPal.']},\n",
       "  {'tag': 'shipping_inquiry',\n",
       "   'patterns': ['When will my order arrive?',\n",
       "    'How long does shipping take?',\n",
       "    'What is the estimated delivery time?',\n",
       "    'Do you offer express shipping?',\n",
       "    'How can I track my shipment?',\n",
       "    'What are the shipping options?',\n",
       "    'Where is my package?',\n",
       "    'When will my order be delivered?',\n",
       "    'Is shipping free?',\n",
       "    'Do you ship internationally?'],\n",
       "   'responses': ['Shipping usually takes 5-7 business days.',\n",
       "    'You can track your order on our tracking page.',\n",
       "    'We offer both standard and express shipping options.',\n",
       "    'Yes, we do offer international shipping!']},\n",
       "  {'tag': 'feedback',\n",
       "   'patterns': ['How can I leave feedback?',\n",
       "    'Can I submit a review?',\n",
       "    'Where can I leave a comment?',\n",
       "    'How do I share my feedback?',\n",
       "    'Can I rate a product?',\n",
       "    'Is there a feedback form?',\n",
       "    'Where can I write a review?',\n",
       "    'I want to share my experience',\n",
       "    'How do I leave a review?',\n",
       "    'How can I give you feedback?'],\n",
       "   'responses': ['Weâ€™d love to hear your feedback! You can leave a review on our website.',\n",
       "    'You can submit feedback through our contact page.',\n",
       "    'Feel free to leave a review on the product page!',\n",
       "    'You can rate and review products after purchasing them.']},\n",
       "  {'tag': 'help',\n",
       "   'patterns': ['I need help',\n",
       "    'Can you help me?',\n",
       "    'Can you assist me?',\n",
       "    'I donâ€™t understand',\n",
       "    'What do I do now?',\n",
       "    'Iâ€™m stuck',\n",
       "    'I need guidance',\n",
       "    'Can you explain?',\n",
       "    'Help me out',\n",
       "    'What do I do next?'],\n",
       "   'responses': ['Sure, what do you need help with?',\n",
       "    'Iâ€™m here to help! Whatâ€™s the issue?',\n",
       "    'Let me know how I can assist you.',\n",
       "    \"I'm happy to help. What do you need help with?\"]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing the patterns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ACER/nltk_data'\n    - 'c:\\\\Users\\\\ACER\\\\OneDrive\\\\Desktop\\\\model_venv\\\\model\\\\nltk_data'\n    - 'c:\\\\Users\\\\ACER\\\\OneDrive\\\\Desktop\\\\model_venv\\\\model\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ACER\\\\OneDrive\\\\Desktop\\\\model_venv\\\\model\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ACER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m             training_data\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mdict\u001b[39m([(token, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]), tag))\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_data\n\u001b[1;32m---> 10\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_traindata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m, in \u001b[0;36mget_traindata\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      4\u001b[0m     tag \u001b[38;5;241m=\u001b[39m intent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m intent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatterns\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 6\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         training_data\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mdict\u001b[39m([(token, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]), tag))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m training_data\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalnum() \u001b[38;5;129;01mand\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\ACER\\OneDrive\\Desktop\\model_venv\\model\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ACER\\OneDrive\\Desktop\\model_venv\\model\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\OneDrive\\Desktop\\model_venv\\model\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\OneDrive\\Desktop\\model_venv\\model\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\OneDrive\\Desktop\\model_venv\\model\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\ACER\\OneDrive\\Desktop\\model_venv\\model\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ACER/nltk_data'\n    - 'c:\\\\Users\\\\ACER\\\\OneDrive\\\\Desktop\\\\model_venv\\\\model\\\\nltk_data'\n    - 'c:\\\\Users\\\\ACER\\\\OneDrive\\\\Desktop\\\\model_venv\\\\model\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ACER\\\\OneDrive\\\\Desktop\\\\model_venv\\\\model\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ACER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def get_traindata(data):\n",
    "    training_data = []\n",
    "    for intent in data['intents']:\n",
    "        tag = intent['tag']\n",
    "        for pattern in intent['patterns']:\n",
    "            tokens = preprocess_text(pattern)\n",
    "            training_data.append((dict([(token, True) for token in tokens]), tag))\n",
    "    return training_data\n",
    "\n",
    "training_data = get_traindata(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining_data\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39mNaiveBayesClassifier\u001b[38;5;241m.\u001b[39mtrain(\u001b[43mtraining_data\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model=NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(data,tag):\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag']==tag:\n",
    "         return random.choice(intent['responses'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inten(text):\n",
    "    tokens=preprocess_text(text)\n",
    "    features=dict([(token,True) for token in tokens])\n",
    "    tag=model.classify(features)\n",
    "    respond=get_response(data,tag)\n",
    "    return respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_inten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_inten\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_inten' is not defined"
     ]
    }
   ],
   "source": [
    "print(get_inten('k'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
